# ðŸš€ Luma Events Automation System For Raga.finance



## ðŸŒŸ Features
- **Automatic Google Sheet Generation** from Luma events using Scraping
- **Smart Social Media Linking** (Twitter/Instagram/LinkedIn/YouTube/TikTok)


## Project Structure

```

raga.finance_luma/
â”œâ”€â”€ __pycache__/            
â”œâ”€â”€ venv/                     
â”œâ”€â”€ .env                (need to be created)
â”œâ”€â”€ credentials.json         
â”œâ”€â”€ gsheet_drive.py         
â”œâ”€â”€ luma_scraper.py         
â”œâ”€â”€ main.py                             
|    readme.md              
â”œâ”€â”€ scrape_attendees.py      

```

## ðŸš€ 1. For Setup
###  First ` Clone the repository`

   ## -> Create `.env` file with these exact values:
   ```bash
   EMAIL="examaple@gmail.com" // change with yours luma email

   PASSWORD="luma@123dhedhe" // your actual password

   EVENT_URL="https://lu.ma/pakav9at?tk=AzhgsJ"   // just paste the event url you want to scrap
   EVENT_TYPE="upcoming"  // either upcoming or past


   ```



   ### Add `credentials.json` file  (provided / download through drive link )
   

   

## 2. Activating the Virtual Environment

Make sure you're in the project directory (`raga.finance_luma`) before running these commands.

### ðŸªŸ On Windows (CMD)
```
venv\Scripts\activate
```

### ðŸ’» On macOS / Linux / Windows (PowerShell)
```
source venv/bin/activate
```


## 3. Running the Project

Ensure your virtual environment is **activated** and both the `.env` and `credentials.json` files are correctly configured.

Then execute the main script:

```
python main.py
```

This script will:
- Authenticate and log into your Luma account
- Scrape attendee details from the specified event(s)
- Automatically create and populate a **Google Sheet** with the extracted data


âœ… **Final Output:**  
> Once the process completes, **both the Google Sheet link and the Google Drive folder link** will be generated in the terminal. These links allow you to view and manage all the event data youâ€™ve scraped â€” neatly organized and stored.



> **Note:** To avoid session timeout issues during longer scraping operations, it is recommended to increase your systemâ€™s screen timeout or sleep settings if possible
